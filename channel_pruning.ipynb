{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c0cebff-47ef-4b4d-8009-8b1106edb668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T02:00:00.666182Z",
     "iopub.status.busy": "2024-12-07T02:00:00.665593Z",
     "iopub.status.idle": "2024-12-07T02:00:04.250363Z",
     "shell.execute_reply": "2024-12-07T02:00:04.249793Z",
     "shell.execute_reply.started": "2024-12-07T02:00:00.666148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.16.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: thop in /usr/local/lib/python3.11/dist-packages (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch_pruning in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.1.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_pruning) (1.26.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->thop) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# Install PyTorch with MPS support\n",
    "!pip install torch torchvision\n",
    "# Install thops for metric analysis\n",
    "!pip install thop torch_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4178b4c-33f9-45e8-a922-8dbd13636667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T02:00:04.251725Z",
     "iopub.status.busy": "2024-12-07T02:00:04.251470Z",
     "iopub.status.idle": "2024-12-07T02:00:04.255142Z",
     "shell.execute_reply": "2024-12-07T02:00:04.254605Z",
     "shell.execute_reply.started": "2024-12-07T02:00:04.251706Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torch_pruning as tp  # Torch-Pruning library\n",
    "import thop  # For FLOPs and parameter counting\n",
    "import utils.dependencies as utils  # Ensure this module exists and contains necessary functions\n",
    "import utils.metrics as metri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc76e1c1-fa77-46ed-962b-788fd884dbc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T02:00:04.255954Z",
     "iopub.status.busy": "2024-12-07T02:00:04.255801Z",
     "iopub.status.idle": "2024-12-07T02:00:04.259385Z",
     "shell.execute_reply": "2024-12-07T02:00:04.258928Z",
     "shell.execute_reply.started": "2024-12-07T02:00:04.255936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base model configuration\n",
    "BASE_MODEL_NAME = 'efficientnet_b0'  # Change this to switch models (e.g., 'resnet18', 'vgg16')\n",
    "NUM_CLASSES = 10  # Adjust based on your dataset (e.g., 10 for CIFAR10)\n",
    "\n",
    "# Pruning configuration\n",
    "PRUNING_METHOD = 'channel pruning'  # Options: 'l1_unstructured', 'random_unstructured', etc.\n",
    "PRUNING_RATIO = 0.2  # Percentage of weights to prune (e.g., 0.2 for 20%)\n",
    "\n",
    "# Paths\n",
    "STATE_DICT_PATH = './models/finetuned_base_model.pth'  # Path to the saved state dictionary\n",
    "SAVE_DIR = './pruned_models/'  # Directory to save pruned models and metrics\n",
    "\n",
    "# Ensure SAVE_DIR exists\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ecd9fcd-596b-4f7f-9804-3b8a152541f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T02:00:04.264128Z",
     "iopub.status.busy": "2024-12-07T02:00:04.263735Z",
     "iopub.status.idle": "2024-12-07T02:00:04.531802Z",
     "shell.execute_reply": "2024-12-07T02:00:04.531343Z",
     "shell.execute_reply.started": "2024-12-07T02:00:04.264109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.efficientnet_b0()\n",
    "in_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(in_features, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(STATE_DICT_PATH, map_location=device)\n",
    "# Load state dict into the model\n",
    "model.load_state_dict(state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bf0b6-e33a-478c-9543-c52542105b56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T02:00:04.532568Z",
     "iopub.status.busy": "2024-12-07T02:00:04.532421Z",
     "iopub.status.idle": "2024-12-07T02:00:04.916270Z",
     "shell.execute_reply": "2024-12-07T02:00:04.915681Z",
     "shell.execute_reply.started": "2024-12-07T02:00:04.532555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Sparsity: 0.00%\n",
      "Applied 36.0% channel-level pruning using L1 norm.\n",
      "Sparsity after channel pruning: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def calculate_sparsity(model):\n",
    "    \"\"\"\n",
    "    Calculates the sparsity (percentage of zero weights) in the model's Conv2d and Linear layers.\n",
    "    \"\"\"\n",
    "    total_weights = 0\n",
    "    zero_weights = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            weight = module.weight.data.cpu().numpy()\n",
    "            total_weights += weight.size\n",
    "            zero_weights += (weight == 0).sum()\n",
    "    sparsity = 100.0 * zero_weights / total_weights if total_weights > 0 else 0\n",
    "    return sparsity\n",
    "\n",
    "def apply_channel_pruning(model, pruning_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Applies channel-level pruning to the given model using L1-norm based importance.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to prune.\n",
    "        pruning_ratio (float): The fraction of channels to prune globally (e.g., 0.2 for 20%).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The pruned model.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()  \n",
    "    # Create a dummy input tensor matching the input shape expected by the model\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(next(model.parameters()).device)  \n",
    "    # Define the importance metric: L1 norm\n",
    "    importance = tp.importance.MagnitudeImportance(p=1) \n",
    "    # Identify layers to ignore during pruning (e.g., final classifier layer)\n",
    "    ignored_layers = []\n",
    "    for m in model.modules():\n",
    "        # Avoid the final classifier layer\n",
    "        if isinstance(m, nn.Linear) and m.out_features == 10:\n",
    "            ignored_layers.append(m)  \n",
    "    # Initialize the MetaPruner with the desired settings\n",
    "    pruner = tp.pruner.MetaPruner(\n",
    "        model=model,\n",
    "        example_inputs=dummy_input,\n",
    "        importance=importance,\n",
    "        pruning_ratio=pruning_ratio,  \n",
    "        ignored_layers=ignored_layers,\n",
    "        #For hardware acceleration\n",
    "        round_to=8,  \n",
    "    )\n",
    "    # Perform pruning\n",
    "    pruner.step()\n",
    "    print(f\"Applied {pruning_ratio * 100:.1f}% channel-level pruning using L1 norm.\")\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "initial_sparsity = calculate_sparsity(model)\n",
    "print(f\"Initial Sparsity: {initial_sparsity:.2f}%\")\n",
    "\n",
    "model = apply_channel_pruning(model, pruning_ratio=PRUNING_RATIO)\n",
    "\n",
    "post_pruning_sparsity = calculate_sparsity(model)\n",
    "print(f\"Sparsity after channel pruning: {post_pruning_sparsity:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72602d16-4023-4f03-9031-5de244d8123d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T02:00:04.917120Z",
     "iopub.status.busy": "2024-12-07T02:00:04.916926Z",
     "iopub.status.idle": "2024-12-07T02:00:04.953485Z",
     "shell.execute_reply": "2024-12-07T02:00:04.952847Z",
     "shell.execute_reply.started": "2024-12-07T02:00:04.917120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model saved at ./pruned_models/channel_pruned_model.pth.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to save the pruned model\n",
    "PRUNED_STATE_DICT_PATH = os.path.join(SAVE_DIR, 'channel_pruned_model.pth')\n",
    "\n",
    "# Save the pruned model's state dictionary\n",
    "torch.save(model.state_dict(), PRUNED_STATE_DICT_PATH)\n",
    "print(f\"Pruned model saved at {PRUNED_STATE_DICT_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2e69e74-e207-4db0-99e1-a27f3e908291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T02:00:04.954388Z",
     "iopub.status.busy": "2024-12-07T02:00:04.954217Z",
     "iopub.status.idle": "2024-12-07T02:00:04.961273Z",
     "shell.execute_reply": "2024-12-07T02:00:04.960677Z",
     "shell.execute_reply.started": "2024-12-07T02:00:04.954377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze all layers except the classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "# Define the layers to unfreeze (last two blocks)\n",
    "layers_to_unfreeze = ['features.5', 'features.6', 'features.7']\n",
    "\n",
    "# Unfreeze the specified layers\n",
    "utils.unfreeze_layers(model, layers_to_unfreeze)\n",
    "\n",
    "# Define optimizer to include only trainable parameters\n",
    "optimizer = optim.SGD(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=0.09,  \n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "# Define a learning rate scheduler for fine-tuning\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e0b2aad-1c14-45d9-9c02-02242f5a9715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T02:00:04.962245Z",
     "iopub.status.busy": "2024-12-07T02:00:04.962103Z",
     "iopub.status.idle": "2024-12-07T02:16:22.770999Z",
     "shell.execute_reply": "2024-12-07T02:16:22.770391Z",
     "shell.execute_reply.started": "2024-12-07T02:00:04.962230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "--- Epoch 1 ---\n",
      "Train Loss: 0.9023 | Train Acc: 68.58% | Time: 44.40s\n",
      "Val Loss: 0.7390 | Val Acc: 73.30%\n",
      "Best model saved with Val Acc: 73.30%\n",
      "\n",
      "--- Epoch 2 ---\n",
      "Train Loss: 0.6358 | Train Acc: 77.92% | Time: 44.72s\n",
      "Val Loss: 0.5865 | Val Acc: 79.50%\n",
      "Best model saved with Val Acc: 79.50%\n",
      "\n",
      "--- Epoch 3 ---\n",
      "Train Loss: 0.5619 | Train Acc: 80.55% | Time: 44.38s\n",
      "Val Loss: 0.6582 | Val Acc: 77.68%\n",
      "No improvement this epoch.\n",
      "\n",
      "--- Epoch 4 ---\n",
      "Train Loss: 0.5363 | Train Acc: 81.29% | Time: 44.19s\n",
      "Val Loss: 0.5521 | Val Acc: 80.44%\n",
      "Best model saved with Val Acc: 80.44%\n",
      "\n",
      "--- Epoch 5 ---\n",
      "Train Loss: 0.5196 | Train Acc: 82.05% | Time: 44.61s\n",
      "Val Loss: 0.5532 | Val Acc: 81.30%\n",
      "Best model saved with Val Acc: 81.30%\n",
      "\n",
      "--- Epoch 6 ---\n",
      "Train Loss: 0.5174 | Train Acc: 82.14% | Time: 44.71s\n",
      "Val Loss: 0.6249 | Val Acc: 78.86%\n",
      "No improvement this epoch.\n",
      "\n",
      "--- Epoch 7 ---\n",
      "Train Loss: 0.3564 | Train Acc: 87.69% | Time: 45.18s\n",
      "Val Loss: 0.3854 | Val Acc: 86.38%\n",
      "Best model saved with Val Acc: 86.38%\n",
      "\n",
      "--- Epoch 8 ---\n",
      "Train Loss: 0.2960 | Train Acc: 89.83% | Time: 44.71s\n",
      "Val Loss: 0.3700 | Val Acc: 86.88%\n",
      "Best model saved with Val Acc: 86.88%\n",
      "\n",
      "--- Epoch 9 ---\n",
      "Train Loss: 0.2633 | Train Acc: 90.81% | Time: 44.76s\n",
      "Val Loss: 0.3638 | Val Acc: 87.58%\n",
      "Best model saved with Val Acc: 87.58%\n",
      "\n",
      "--- Epoch 10 ---\n",
      "Train Loss: 0.2478 | Train Acc: 91.53% | Time: 44.31s\n",
      "Val Loss: 0.3623 | Val Acc: 87.32%\n",
      "No improvement this epoch.\n",
      "\n",
      "--- Epoch 11 ---\n",
      "Train Loss: 0.2359 | Train Acc: 91.82% | Time: 44.66s\n",
      "Val Loss: 0.3523 | Val Acc: 87.82%\n",
      "Best model saved with Val Acc: 87.82%\n",
      "\n",
      "--- Epoch 12 ---\n",
      "Train Loss: 0.2189 | Train Acc: 92.38% | Time: 44.78s\n",
      "Val Loss: 0.3619 | Val Acc: 87.44%\n",
      "No improvement this epoch.\n",
      "\n",
      "--- Epoch 13 ---\n",
      "Train Loss: 0.1871 | Train Acc: 93.65% | Time: 44.91s\n",
      "Val Loss: 0.3340 | Val Acc: 88.40%\n",
      "Best model saved with Val Acc: 88.40%\n",
      "\n",
      "--- Epoch 14 ---\n",
      "Train Loss: 0.1771 | Train Acc: 94.08% | Time: 44.83s\n",
      "Val Loss: 0.3439 | Val Acc: 88.06%\n",
      "No improvement this epoch.\n",
      "\n",
      "--- Epoch 15 ---\n",
      "Train Loss: 0.1752 | Train Acc: 94.07% | Time: 45.07s\n",
      "Val Loss: 0.3336 | Val Acc: 88.32%\n",
      "No improvement this epoch.\n",
      "\n",
      "--- Epoch 16 ---\n",
      "Train Loss: 0.1722 | Train Acc: 94.13% | Time: 44.25s\n",
      "Val Loss: 0.3356 | Val Acc: 88.76%\n",
      "Best model saved with Val Acc: 88.76%\n",
      "\n",
      "--- Epoch 17 ---\n",
      "Train Loss: 0.1677 | Train Acc: 94.25% | Time: 44.45s\n",
      "Val Loss: 0.3427 | Val Acc: 88.56%\n",
      "No improvement this epoch.\n",
      "\n",
      "--- Epoch 18 ---\n",
      "Train Loss: 0.1648 | Train Acc: 94.45% | Time: 44.01s\n",
      "Val Loss: 0.3400 | Val Acc: 88.22%\n",
      "No improvement this epoch.\n",
      "\n",
      "--- Epoch 19 ---\n",
      "Train Loss: 0.1621 | Train Acc: 94.66% | Time: 44.71s\n",
      "Val Loss: 0.3450 | Val Acc: 88.12%\n",
      "No improvement this epoch.\n",
      "\n",
      "--- Epoch 20 ---\n",
      "Train Loss: 0.1586 | Train Acc: 94.73% | Time: 44.95s\n",
      "Val Loss: 0.3376 | Val Acc: 88.48%\n",
      "No improvement this epoch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "save_path = f'./models/finegrained_prune_ratio_{PRUNING_RATIO}'\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Get the data loaders\n",
    "train_loader, val_loader, test_loader = utils.get_data_loaders()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"--- Epoch {epoch} ---\")    \n",
    "    # Train\n",
    "    train_loss, train_acc = utils.train_epoch(model, device, train_loader, optimizer)  \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    # Validate\n",
    "    val_loss, val_acc = utils.validate_epoch(model, device, val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    # Step the scheduler\n",
    "    scheduler.step()    \n",
    "    # Save the best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Best model saved with Val Acc: {best_val_acc:.2f}%\\n\")\n",
    "    else:\n",
    "        print(\"No improvement this epoch.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af241bd3-edca-41da-833a-6b22ec027963",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T02:16:22.772490Z",
     "iopub.status.busy": "2024-12-07T02:16:22.771956Z",
     "iopub.status.idle": "2024-12-07T02:16:31.196232Z",
     "shell.execute_reply": "2024-12-07T02:16:31.195679Z",
     "shell.execute_reply.started": "2024-12-07T02:16:22.772490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Metrics for channel_pruning_0.36 ---\n",
      "Calculating Model Size...\n",
      "Measuring Inference Time...\n",
      "Computing FLOPs...\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Measuring Memory Usage...\n",
      "Evaluating Model on Test Set...\n",
      "Test Loss: 0.3720 | Test Accuracy: 87.65%\n",
      "Generating Metrics Table...\n",
      "Metrics table saved in 'metrics_plots' as 'channel_pruning_0.36_metrics_table.png'.\n",
      "Generating Training and Validation Metrics Plots...\n",
      "Training and validation metrics plots saved in 'metrics_plots' as 'channel_pruning_0.36_training_validation_plots.png'.\n",
      "--- Metrics Generation Completed ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils.metrics as metrics\n",
    "\n",
    "# Generate and save metrics plots and table\n",
    "metrics.generate_and_save_metrics(\n",
    "    model=model, \n",
    "    device=device, \n",
    "    test_loader=test_loader, \n",
    "    criterion=utils.criterion, \n",
    "    model_name=f'channel_pruning_{PRUNING_RATIO}', \n",
    "    pruning_ratio=PRUNING_RATIO, \n",
    "    description= f'Model with channel pruning',\n",
    "    train_losses=train_losses,\n",
    "    train_accuracies=train_accuracies,\n",
    "    val_losses=val_losses,\n",
    "    val_accuracies=val_accuracies,\n",
    "    save_dir='metrics_plots'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
